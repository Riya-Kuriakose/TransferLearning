# -*- coding: utf-8 -*-
"""Transfer_Learning_Vgg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bTDdAOOi2DxjMz6Pue0cjHDIxpd-QtsQ

Importing the  required modules
"""

!pip install keras.utils

import os

import random
import numpy as np
import keras

import matplotlib.pyplot as plt
from matplotlib.pyplot import imshow

import tensorflow

from tensorflow.keras.utils import to_categorical
from sklearn.metrics import f1_score,plot_roc_curve

from keras.preprocessing import image
from keras.applications import vgg16
from keras.applications.imagenet_utils import preprocess_input
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Activation
from keras.layers import Conv2D, MaxPooling2D
from keras.models import Model

"""Importing Caltech10 dataset"""

# Unzipping the dataset
import shutil
shutil.unpack_archive("/content/101_ObjectCategories.tar.gz","/content/data")

# Reading the contents

root = '/content/data/101_ObjectCategories' #specify the path to the root folder
#exclude = ['BACKGROUND_Google', 'Motorbikes', 'airplanes', 'Faces_easy', 'Faces']
include = ['beaver','bass','cannon','emu','kangaroo','lotus','snoopy','starfish','revolver','wrench']
train_split, val_split = 0.7, 0.15

categories = [x[0] for x in os.walk(root) if x[0]][1:]
#categories = [c for c in categories if c  not in [os.path.join(root, e) for e in exclude]]
categories = [c for c in categories if c   in [os.path.join(root, i) for i in include]]

print(categories)

# Getting the images

def get_image(path):
    img = image.load_img(path, target_size=(224, 224))
   # print(img)
    x = image.img_to_array(img)
  #  x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)
    return x

data = []
for c, category in enumerate(categories):
    images = [os.path.join(dp, f) for dp, dn, filenames 
             in os.walk(category) for f in filenames 
             if os.path.splitext(f)[1].lower() in ['.jpg','.png','.jpeg']]
    print(images)
    for img_path in images:
        x = get_image(img_path)
        data.append({'x':x,'y':c})
print(x.shape)
print(data)

# count the number of classes
num_classes = len(categories)
print(num_classes)

#Shuffling the data
random.shuffle(data)

#Splitting into train,test and validation
idx_val = int(train_split * len(data))
idx_test = int((train_split + val_split) * len(data))
train = data[:idx_val]
val = data[idx_val:idx_test]
test = data[idx_test:]

print(train)

x_train, y_train = np.array([t["x"] for t in train]), [t["y"] for t in train]
x_val, y_val = np.array([t["x"] for t in val]), [t["y"] for t in val]
x_test, y_test = np.array([t["x"] for t in test]), [t["y"] for t in test]
print(y_test)

# normalize data
x_train = x_train.astype('float32') / 255.
x_val = x_val.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.

# convert labels to one-hot vectors
y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)
y_val = tensorflow.keras.utils.to_categorical(y_val, num_classes)
y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)
print(y_test.shape)

# summary
print("finished loading %d images from %d categories"%(len(data), num_classes))
print("train / validation / test split: %d, %d, %d"%(len(x_train), len(x_val), len(x_test)))
print("training data shape: ", x_train.shape)
print("training labels shape: ", y_train.shape)
print("validation data shape:",x_val.shape)

images = [os.path.join(dp, f) for dp, dn, filenames in os.walk(root) for f in filenames if os.path.splitext(f)[1].lower() in ['.jpg','.png','.jpeg']]
idx = [int(len(images) * random.random()) for i in range(8)]
imgs = [image.load_img(images[i], target_size=(224, 224)) for i in idx]
concat_image = np.concatenate([np.asarray(img) for img in imgs], axis=1)
plt.figure(figsize=(16,4))

plt.imshow(concat_image)



# build the network

#x_train.reshape(x_train.shape[0],224,224,3)
Input_shape=(224,224,3)
model = Sequential()
#print("Input dimensions: ",Input_shape)

model.add(Conv2D(32, (3, 3), input_shape=x_train.shape[1:]))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Dropout(0.25))

model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))

model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(256))
model.add(Activation('relu'))

model.add(Dropout(0.5))

model.add(Dense(num_classes))
model.add(Activation('softmax'))

model.summary()

# compile the model to use categorical cross-entropy loss function and adadelta optimizer
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model.fit(x_train, y_train,
                    batch_size=128,
                    epochs=10,
                    validation_data=(x_val, y_val))

fig = plt.figure(figsize=(16,4))
ax = fig.add_subplot(121)
ax.plot(history.history["val_loss"])
ax.set_title("validation loss")
ax.set_xlabel("epochs")

ax2 = fig.add_subplot(122)
ax2.plot(history.history["val_accuracy"])
ax2.set_title("validation accuracy")
ax2.set_xlabel("epochs")
ax2.set_ylim(0, 1)

loss, accuracy = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', loss)
print('Test accuracy:', accuracy)

#downloading vgg16
vgg = keras.applications.vgg16.VGG16(weights='imagenet', include_top=True)
vgg.summary()

# make a reference to VGG's input layer
inp = vgg.input

# make a new softmax layer with num_classes neurons
new_classification_layer = Dense(num_classes, activation='softmax')

# connect our new layer to the second to last layer in VGG, and make a reference to it
out = new_classification_layer(vgg.layers[-2].output)

# create a new network between inp and out
model_new = Model(inp, out)

# make all layers untrainable by freezing weights (except for last layer)
for l, layer in enumerate(model_new.layers[:-1]):
    layer.trainable = False

# ensure the last layer is trainable/not frozen
for l, layer in enumerate(model_new.layers[-1:]):
    layer.trainable = True

model_new.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

model_new.summary()

# make all layers untrainable by freezing weights (except for  last two layers)
for l, layer in enumerate(model_new.layers[:-2]):
    layer.trainable = False

# ensure the last layer is trainable/not frozen
for l, layer in enumerate(model_new.layers[-2:]):
    layer.trainable = True

model_new.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

model_new.summary()

#make all layers untrainable by freezing weights (except for  last three layers)
for l, layer in enumerate(model_new.layers[:-3]):
    layer.trainable = False

# ensure the last layer is trainable/not frozen
for l, layer in enumerate(model_new.layers[-3:]):
    layer.trainable = True

model_new.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

model_new.summary()

history2 = model_new.fit(x_train, y_train, 
                         batch_size=128, 
                         epochs=10, 
                         validation_data=(x_val, y_val))

fig = plt.figure(figsize=(16,4))
ax = fig.add_subplot(121)
ax.plot(history.history["val_loss"])
ax.plot(history2.history["val_loss"])
ax.set_title("validation loss")
ax.set_xlabel("epochs")

ax2 = fig.add_subplot(122)
ax2.plot(history.history["val_accuracy"])
ax2.plot(history2.history["val_accuracy"])
ax2.set_title("validation accuracy")
ax2.set_xlabel("epochs")
ax2.set_ylim(0, 1)

plt.show()

from sklearn.metrics import  classification_report
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import roc_curve

rounded_predictions = model_new.predict(x_test,batch_size=64,verbose=0)
y_pred_bool = np.argmax(y_pred,axis=1)

import numpy as np
rounded_labels=np.argmax(y_pred_bools, axis=1)
rounded_labels[1]

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(rounded_labels, rounded_predictions)
cm
print(classification_report(y_test,y_pred_bool))
loss, accuracy = model_new.evaluate(x_test, y_test, verbose=0)
#print('F1 Score',model_new.f1_score(x_test,y_test))
#metrics.plot_roc_curve(model_new,x_test,y_test)

print('Test loss:', loss)
print('Test accuracy:', accuracy)





# ReInitializing the last three layers and training the new model

from keras import initializers
inp = vgg.input


initializer = initializers.RandomNormal(mean=0., stddev=1.)
layer_fc1 = Dense(4096, kernel_initializer=initializer)

layer_fc2 = Dense(4096, kernel_initializer=initializer)

layer_predicition = Dense(97, kernel_initializer=initializer)

output1 = layer_fc1(vgg.layers[-4].output)
model_new = Model(inp, out)

output2 = layer_fc2(vgg.layers[-3].output)
model_new = Model(inp, out_1)


output3 = layer_predicition(vgg.layers[-2].output)
model_new = Model(inp, out_2)